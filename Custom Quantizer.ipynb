{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d727a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7a3c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goal is to build a Simple Linear Layer class where weights are stored in 8-bits we upcaste them \\nto inputs dtype in forward'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Goal is to build a Simple Linear Layer class where weights are stored in 8-bits we upcaste them \n",
    "to inputs dtype in forward\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b41f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f5a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3f023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weights,inputs,scales,bias=None):\n",
    "    casted_weights = weights.to(inputs.dtype)\n",
    "    out = F.linear(inputs,casted_weights) * scales\n",
    "    if bias is not None:\n",
    "        out = out + bias\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5015d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    #make init similar to nn.Linear\n",
    "    def __init__(self,in_features,out_features,bias=None,dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        #create params using register buffer\n",
    "        self.register_buffer(\"int8_weights\",torch.randint(-128,127,(out_features,in_features),dtype=torch.int8))\n",
    "        self.register_buffer(\"scales\",torch.randn((out_features),dtype=dtype))\n",
    "        #bias\n",
    "        if bias is not None:\n",
    "            self.register_buffer(\"bias\",torch.randn((1,out_features),dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "    def quantize(self,weights):\n",
    "        #upcast weights for stability!?\n",
    "        wp_32 = weights.clone().to(torch.float32)\n",
    "        scales = wp_32.abs().max(dim=-1).values/127\n",
    "        scales = scales.to(weights.dtype)\n",
    "        int8_weights = torch.round(weights/scales.unsqueeze(1)).to(torch.int8)\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    def forward(self,inputs):\n",
    "        return w8_a16_forward(self.int8_weights,inputs,self.scales,self.bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d90626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(8,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d057e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  38,   34,   96,   15, -118,  -11,  -37,   45],\n",
       "        [ 115, -106, -109,   21,  -81, -122,   46,  -65],\n",
       "        [  59,  126,  -47,   32,  -22,   70,   88, -124],\n",
       "        [ -35,  -31,   52,  -38,  -49,   33,  105,  -13],\n",
       "        [  74, -100,  -64,  -43,  -29,  -10,  -98,  -74],\n",
       "        [  68,   61,   60,  -64,   95,  -86,  -76,  -48],\n",
       "        [ -57,   19,  -61,  -82,  -70,  -54, -107, -111],\n",
       "        [ -32, -114,  -92,  -28,   49,   36,  -81,  -12],\n",
       "        [  18,   57,  -25,  -28,    7,   16,   75, -101],\n",
       "        [ 120,  -59,  121,   26,  -16,   59, -122,   -9],\n",
       "        [-100,  116,  -24,   44,  -88,   94,  105,  -74],\n",
       "        [-126, -101,   45,   56,  -74,  -84,   68,  -81],\n",
       "        [ 110,  -88,   81,  -20,  -24,  -74, -105,   99],\n",
       "        [  34,   92,  -71, -102,   29,  -29,  -82,  108],\n",
       "        [  19,    2,  -74,  -77,   76,   46,  -80, -112],\n",
       "        [-127,   17,  122, -117,   48,  -61,   67,  -69]], dtype=torch.int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.int8_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39097ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4381,  0.0704,  1.0747,  0.0724, -0.8331, -0.4196, -1.5668,  1.5509,\n",
       "        -0.2814,  0.0656, -0.2010,  0.6287,  1.2702, -0.3286,  0.9354,  1.8299])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a128c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_init = W8A16LinearLayer(16,32,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fe1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(16,32)\n",
    "dummy_hidden_state = torch.randn(1,6,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95e400c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_state).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1052d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.int8_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13676617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_quant_weights(module,target_class,module_name_to_exclude):\n",
    "    for name,child in module.named_children():\n",
    "        if isinstance(child,nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            new_module = target_class(child.in_features,child.out_features,\n",
    "                                      old_bias is not None,\n",
    "                                      child.weight.dtype)\n",
    "            setattr(module,name,new_module)\n",
    "            if old_bias is not None:\n",
    "                getattr(module,name).bias = old_bias\n",
    "        else:\n",
    "            #recurssively call func for nested ones\n",
    "            replace_quant_weights(child,target_class,module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f418aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(1, 1)\n",
    "        # Try with bias\n",
    "        self.linear_1 = nn.Linear(1, 1)\n",
    "        # Try without bias\n",
    "        self.linear_2 = nn.Linear(1, 1, bias=False)\n",
    "        # Lm prediction head\n",
    "        self.lm_head = nn.Linear(1, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c245f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = DummyModel()\n",
    "model_2 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ac3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_quant_weights(model_1, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36848ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): W8A16LinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_quant_weights(model_2, W8A16LinearLayer, [])\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b751a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_quant_weights_and_quant(module,target_class,module_name_to_exclude):\n",
    "    for name,child in module.named_children():\n",
    "        if isinstance(child,nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "            new_module = target_class(child.in_features,child.out_features,\n",
    "                                      old_bias is not None,\n",
    "                                      child.weight.dtype)\n",
    "            setattr(module,name,new_module)\n",
    "            #quantize\n",
    "            getattr(module,name).quantize(old_weight)\n",
    "            if old_bias is not None:\n",
    "                getattr(module,name).bias = old_bias\n",
    "        else:\n",
    "            #recurssively call func for nested ones\n",
    "            replace_quant_weights_and_quant(child,target_class,module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee36c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064c3a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): W8A16LinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_quant_weights_and_quant(model_3, W8A16LinearLayer, [])\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce58398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"Salesforce/codegen-350M-mono\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                    torch_dtype=torch.bfloat16, \n",
    "                                             low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc6034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c90e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'def hello_world():\\n    print(\"Hello World\")\\n\\nhello_world()\\n\\n# 파'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21e56de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before:\n",
      "\n",
      " CodeGenForCausalLM(\n",
      "  (transformer): CodeGenModel(\n",
      "    (wte): Embedding(51200, 1024)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-19): 20 x CodeGenBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CodeGenAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): CodeGenMLP(\n",
      "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Model before:\\n\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25449f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_quant_weights_and_quant(model, W8A16LinearLayer, [\"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11816df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world(): \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"\n"
     ]
    }
   ],
   "source": [
    "print(pipe(\"def hello_world():\", max_new_tokens=20, \n",
    "           do_sample=False)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca11b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
